{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itamarbeeri/gittry/blob/master/build_error2size_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir='/content/drive/My Drive/project with Roi Livni/data'\n",
        "output_size = 10\n",
        "epochs = 200\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "momentum = 0.5\n",
        "train_size=50000\n",
        "test_size=10000\n",
        "\n",
        "dataset_names = ['MNIST'] #, 'CIFAR10']\n",
        "hidden_layer_sizes = [8, 128]\n",
        "my_transforms = transforms.Compose([transforms.Resize((10, 10)), ToTensor()])\n"
      ],
      "metadata": {
        "id": "WYuED_GWpcaB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWdP6XVm0DnR",
        "outputId": "ed03d3cf-1d2e-44ae-e586-93b142428ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mount drive..\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "print('mount drive..')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "input dot weights of same size\n",
        "relu\n",
        "dot hidden layer of some size\n",
        "output size\n",
        "turcated softmax\n",
        "total params = H + input size\n",
        "\n",
        "cross entropy loss\n",
        "SGD with momentum and diminishing step size\n",
        "minibatches 100\n",
        "init step size 0.1 with update rule\n",
        "init momentum 0.5 with update rule\n",
        "\n",
        "\n",
        "downsample to 100 pixels\n",
        "50000 MNIST 40000 CIFAR\n",
        "10000 validation\n",
        "\n",
        "increasing H will reduse training and test error initially and then overfit - increase test error.\n"
      ],
      "metadata": {
        "id": "3zDYys_YLIcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYrLDbE2QmXk",
        "outputId": "284ae2aa-28a1-4762-f0b6-2a340813d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imports and initialization..\n"
          ]
        }
      ],
      "source": [
        "print('imports and initialization..')\n",
        "\n",
        "from os import path, mkdir\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "from torchsummary import summary\n",
        "from functools import reduce\n",
        "import uuid\n",
        "import yaml\n",
        "from pprint import pprint\n",
        "from torch.optim.lr_scheduler import MultiplicativeLR\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('mount GPU..')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No Cuda Available\")\n",
        "\n",
        "\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad2cu38IPZmt",
        "outputId": "7ab32193-8c57-4347-de06-fc7047d0ae3f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mount GPU..\n",
            "Using cuda:0 device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LNN(nn.Module):\n",
        "    def __init__(self, input_shape, hidden_layer_size, output_size=10):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        input_layer_size = reduce((lambda x, y: x * y), input_shape)\n",
        "        self.architecture = nn.Sequential(\n",
        "                        nn.Linear(input_layer_size, hidden_layer_size),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(hidden_layer_size, output_size),\n",
        "                        nn.Softmax())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.architecture(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "9MxKZ6IkkAoQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "R38Qd6-yRNM1"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, hidden_layer_size, output_size, learning_rate, momentum):\n",
        "    model = LNN(input_shape, hidden_layer_size, output_size).to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    scheduler = MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.99)\n",
        "\n",
        "    return model, loss_fn, optimizer, scheduler\n",
        "\n",
        "\n",
        "def build_data(dataset_name='MNIST', my_transforms=None, batch_size=32, train_size=60000, test_size=10000):\n",
        "    Data = list()\n",
        "    dataset = datasets.CIFAR10 if dataset_name == 'CIFAR10' else datasets.MNIST\n",
        "\n",
        "    for is_train, size in zip([True, False], [train_size, test_size]):\n",
        "        data = dataset(root=\"data\", train=is_train, download=True, transform=my_transforms)\n",
        "        used_data, _ = torch.utils.data.random_split(data, [size, len(data) - size])\n",
        "        dataloader = DataLoader(used_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "        Data.append((used_data, dataloader))\n",
        "\n",
        "    return Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bY02vvyURkVi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_history(training_history, data_dir, experiment_dict):\n",
        "    dest_dir = path.join(data_dir, experiment_dict['exp_name'])\n",
        "    print(f'saving history to {dest_dir}..')\n",
        "\n",
        "    if path.isdir(dest_dir):\n",
        "        print(f'dir {dest_dir} already exist!')\n",
        "    else:\n",
        "        mkdir(dest_dir)\n",
        "\n",
        "    loss = np.asarray(training_history['loss_history'])\n",
        "    np.save(path.join(dest_dir, f'loss.npy'), loss)\n",
        "\n",
        "    test_loss = np.asarray(training_history['test_loss_history'])\n",
        "    np.save(path.join(dest_dir, f'test_loss.npy'), test_loss)\n",
        "\n",
        "    layers = list(training_history['gradients_history'][0].keys())\n",
        "    for layer in layers:\n",
        "        weights = list()\n",
        "        for batch in training_history['weights_history']:\n",
        "            weights.append(np.asarray(batch[layer].cpu()))\n",
        "        np.save(path.join(dest_dir, f'weights_{layer}.npy'), weights)\n",
        "\n",
        "        gradients = list()\n",
        "        for batch in training_history['gradients_history']:\n",
        "            gradients.append(np.asarray(batch[layer].cpu()))\n",
        "        np.save(path.join(dest_dir, f'gradients_{layer}.npy'), gradients)\n",
        "\n",
        "        test_gradients = list()\n",
        "        for batch in training_history['test_gradients_history']:\n",
        "            test_gradients.append(np.asarray(batch[layer].cpu()))\n",
        "        np.save(path.join(dest_dir, f'test_gradients_{layer}.npy'), test_gradients)\n",
        "\n",
        "    with open(path.join(data_dir, 'experiments.yml'),'r') as file:\n",
        "        experiments = yaml.safe_load(file)\n",
        "        exp_name = experiment_dict.pop('exp_name')\n",
        "        experiments[exp_name] = experiment_dict\n",
        "\n",
        "    with open(path.join(data_dir, 'experiments.yml'), 'w') as file:\n",
        "        yaml.safe_dump(experiments, file)\n",
        "\n",
        "    print(f'data saved in {dest_dir}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zEBj4lYDR-7W"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_dataloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        output = model(images)\n",
        "        loss = loss_fn(output, labels)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss.detach().cpu().item()\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(test_dataloader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model(images)\n",
        "        avg_loss += loss_fn(output, labels).sum()\n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "    avg_loss /= len(test_set)\n",
        "    avg_loss *= batch_size\n",
        "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(test_set)))\n",
        "\n",
        "    return avg_loss.detach().cpu().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oL8StIXSSWvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f7d7284-0afb-40e1-c238-372f826dee3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100,\n",
            " 'dataset_name': 'MNIST',\n",
            " 'epochs': 200,\n",
            " 'exp_name': 'cec73f0e-59da-44e4-b5d4-1b0d7325894b',\n",
            " 'learning_rate': 0.1,\n",
            " 'loss': 'cross_entropy',\n",
            " 'model_summary': 'LNN(\\n'\n",
            "                  '  (flatten): Flatten(start_dim=1, end_dim=-1)\\n'\n",
            "                  '  (architecture): Sequential(\\n'\n",
            "                  '    (0): Linear(in_features=100, out_features=8, '\n",
            "                  'bias=True)\\n'\n",
            "                  '    (1): ReLU()\\n'\n",
            "                  '    (2): Linear(in_features=8, out_features=10, bias=True)\\n'\n",
            "                  '    (3): Softmax(dim=None)\\n'\n",
            "                  '  )\\n'\n",
            "                  ')',\n",
            " 'sample_shape': (1, 10, 10),\n",
            " 'test_set_size': 10000,\n",
            " 'train_set_size': 50000,\n",
            " 'transforms': 'Compose(\\n'\n",
            "               '    Resize(size=(10, 10), interpolation=bilinear, '\n",
            "               'max_size=None, antialias=warn)\\n'\n",
            "               '    ToTensor()\\n'\n",
            "               ')'}\n",
            "run training..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Batch: 0, Loss: 2.303377\n",
            "Epoch 0, Batch: 10, Loss: 2.300911\n",
            "Epoch 0, Batch: 20, Loss: 2.302065\n",
            "Epoch 0, Batch: 30, Loss: 2.301735\n",
            "Epoch 0, Batch: 40, Loss: 2.298720\n",
            "Epoch 0, Batch: 50, Loss: 2.299842\n",
            "Epoch 0, Batch: 60, Loss: 2.301872\n",
            "Epoch 0, Batch: 70, Loss: 2.295636\n",
            "Epoch 0, Batch: 80, Loss: 2.302032\n",
            "Epoch 0, Batch: 90, Loss: 2.300038\n",
            "Epoch 0, Batch: 100, Loss: 2.300833\n",
            "Epoch 0, Batch: 110, Loss: 2.304341\n",
            "Epoch 0, Batch: 120, Loss: 2.296189\n",
            "Epoch 0, Batch: 130, Loss: 2.299995\n",
            "Epoch 0, Batch: 140, Loss: 2.296740\n",
            "Epoch 0, Batch: 150, Loss: 2.295904\n",
            "Epoch 0, Batch: 160, Loss: 2.289294\n",
            "Epoch 0, Batch: 170, Loss: 2.291524\n",
            "Epoch 0, Batch: 180, Loss: 2.291625\n",
            "Epoch 0, Batch: 190, Loss: 2.285177\n",
            "Epoch 0, Batch: 200, Loss: 2.291354\n",
            "Epoch 0, Batch: 210, Loss: 2.283694\n",
            "Epoch 0, Batch: 220, Loss: 2.281820\n",
            "Epoch 0, Batch: 230, Loss: 2.293262\n",
            "Epoch 0, Batch: 240, Loss: 2.299969\n",
            "Epoch 0, Batch: 250, Loss: 2.288693\n",
            "Epoch 0, Batch: 260, Loss: 2.288265\n",
            "Epoch 0, Batch: 270, Loss: 2.278044\n",
            "Epoch 0, Batch: 280, Loss: 2.300108\n",
            "Epoch 0, Batch: 290, Loss: 2.283012\n",
            "Epoch 0, Batch: 300, Loss: 2.275723\n",
            "Epoch 0, Batch: 310, Loss: 2.295352\n",
            "Epoch 0, Batch: 320, Loss: 2.269717\n",
            "Epoch 0, Batch: 330, Loss: 2.244173\n",
            "Epoch 0, Batch: 340, Loss: 2.272681\n",
            "Epoch 0, Batch: 350, Loss: 2.263613\n",
            "Epoch 0, Batch: 360, Loss: 2.279052\n",
            "Epoch 0, Batch: 370, Loss: 2.229629\n",
            "Epoch 0, Batch: 380, Loss: 2.268713\n",
            "Epoch 0, Batch: 390, Loss: 2.268176\n",
            "Epoch 0, Batch: 400, Loss: 2.269503\n",
            "Epoch 0, Batch: 410, Loss: 2.260177\n",
            "Epoch 0, Batch: 420, Loss: 2.253026\n",
            "Epoch 0, Batch: 430, Loss: 2.214284\n",
            "Epoch 0, Batch: 440, Loss: 2.211169\n",
            "Epoch 0, Batch: 450, Loss: 2.248487\n",
            "Epoch 0, Batch: 460, Loss: 2.253886\n",
            "Epoch 0, Batch: 470, Loss: 2.221221\n",
            "Epoch 0, Batch: 480, Loss: 2.268378\n",
            "Epoch 0, Batch: 490, Loss: 2.242133\n",
            "Test Avg. Loss: 2.234689, Accuracy: 0.266800\n",
            "Epoch 1, Batch: 0, Loss: 2.234482\n",
            "Epoch 1, Batch: 10, Loss: 2.204757\n",
            "Epoch 1, Batch: 20, Loss: 2.259721\n",
            "Epoch 1, Batch: 30, Loss: 2.234972\n",
            "Epoch 1, Batch: 40, Loss: 2.224828\n",
            "Epoch 1, Batch: 50, Loss: 2.216767\n",
            "Epoch 1, Batch: 60, Loss: 2.203534\n",
            "Epoch 1, Batch: 70, Loss: 2.200594\n",
            "Epoch 1, Batch: 80, Loss: 2.211022\n",
            "Epoch 1, Batch: 90, Loss: 2.234262\n",
            "Epoch 1, Batch: 100, Loss: 2.203216\n",
            "Epoch 1, Batch: 110, Loss: 2.214204\n",
            "Epoch 1, Batch: 120, Loss: 2.206332\n",
            "Epoch 1, Batch: 130, Loss: 2.207706\n",
            "Epoch 1, Batch: 140, Loss: 2.150373\n",
            "Epoch 1, Batch: 150, Loss: 2.226289\n",
            "Epoch 1, Batch: 160, Loss: 2.157519\n",
            "Epoch 1, Batch: 170, Loss: 2.185277\n",
            "Epoch 1, Batch: 180, Loss: 2.226664\n",
            "Epoch 1, Batch: 190, Loss: 2.197179\n",
            "Epoch 1, Batch: 200, Loss: 2.213798\n",
            "Epoch 1, Batch: 210, Loss: 2.179670\n",
            "Epoch 1, Batch: 220, Loss: 2.133720\n",
            "Epoch 1, Batch: 230, Loss: 2.186653\n",
            "Epoch 1, Batch: 240, Loss: 2.184156\n",
            "Epoch 1, Batch: 250, Loss: 2.162994\n",
            "Epoch 1, Batch: 260, Loss: 2.130055\n",
            "Epoch 1, Batch: 270, Loss: 2.170077\n",
            "Epoch 1, Batch: 280, Loss: 2.182328\n",
            "Epoch 1, Batch: 290, Loss: 2.184052\n",
            "Epoch 1, Batch: 300, Loss: 2.111247\n",
            "Epoch 1, Batch: 310, Loss: 2.123749\n",
            "Epoch 1, Batch: 320, Loss: 2.129874\n",
            "Epoch 1, Batch: 330, Loss: 2.129154\n",
            "Epoch 1, Batch: 340, Loss: 2.097067\n",
            "Epoch 1, Batch: 350, Loss: 2.080324\n",
            "Epoch 1, Batch: 360, Loss: 2.075021\n",
            "Epoch 1, Batch: 370, Loss: 2.085765\n",
            "Epoch 1, Batch: 380, Loss: 2.136295\n",
            "Epoch 1, Batch: 390, Loss: 2.144145\n",
            "Epoch 1, Batch: 400, Loss: 2.081663\n",
            "Epoch 1, Batch: 410, Loss: 2.091733\n",
            "Epoch 1, Batch: 420, Loss: 2.024868\n",
            "Epoch 1, Batch: 430, Loss: 2.064862\n",
            "Epoch 1, Batch: 440, Loss: 2.020979\n",
            "Epoch 1, Batch: 450, Loss: 2.054725\n",
            "Epoch 1, Batch: 460, Loss: 1.999036\n",
            "Epoch 1, Batch: 470, Loss: 2.009536\n",
            "Epoch 1, Batch: 480, Loss: 2.009637\n",
            "Epoch 1, Batch: 490, Loss: 1.980754\n",
            "Test Avg. Loss: 2.006264, Accuracy: 0.547100\n",
            "Epoch 2, Batch: 0, Loss: 1.944831\n",
            "Epoch 2, Batch: 10, Loss: 1.972130\n",
            "Epoch 2, Batch: 20, Loss: 1.992345\n",
            "Epoch 2, Batch: 30, Loss: 1.995741\n",
            "Epoch 2, Batch: 40, Loss: 2.017503\n",
            "Epoch 2, Batch: 50, Loss: 1.978345\n",
            "Epoch 2, Batch: 60, Loss: 1.915972\n",
            "Epoch 2, Batch: 70, Loss: 1.991837\n",
            "Epoch 2, Batch: 80, Loss: 1.887102\n",
            "Epoch 2, Batch: 90, Loss: 1.879279\n",
            "Epoch 2, Batch: 100, Loss: 1.987789\n",
            "Epoch 2, Batch: 110, Loss: 1.941428\n",
            "Epoch 2, Batch: 120, Loss: 1.990436\n",
            "Epoch 2, Batch: 130, Loss: 2.002245\n",
            "Epoch 2, Batch: 140, Loss: 1.987806\n",
            "Epoch 2, Batch: 150, Loss: 1.873776\n",
            "Epoch 2, Batch: 160, Loss: 1.936999\n",
            "Epoch 2, Batch: 170, Loss: 1.960873\n",
            "Epoch 2, Batch: 180, Loss: 1.859288\n",
            "Epoch 2, Batch: 190, Loss: 1.990479\n",
            "Epoch 2, Batch: 200, Loss: 1.926806\n",
            "Epoch 2, Batch: 210, Loss: 1.984728\n",
            "Epoch 2, Batch: 220, Loss: 1.961665\n",
            "Epoch 2, Batch: 230, Loss: 1.982972\n",
            "Epoch 2, Batch: 240, Loss: 1.963797\n",
            "Epoch 2, Batch: 250, Loss: 1.890227\n",
            "Epoch 2, Batch: 260, Loss: 1.900210\n",
            "Epoch 2, Batch: 270, Loss: 1.924922\n",
            "Epoch 2, Batch: 280, Loss: 1.915208\n",
            "Epoch 2, Batch: 290, Loss: 1.907538\n",
            "Epoch 2, Batch: 300, Loss: 1.956058\n",
            "Epoch 2, Batch: 310, Loss: 1.913048\n",
            "Epoch 2, Batch: 320, Loss: 1.964656\n",
            "Epoch 2, Batch: 330, Loss: 1.940417\n",
            "Epoch 2, Batch: 340, Loss: 1.913847\n",
            "Epoch 2, Batch: 350, Loss: 1.981331\n",
            "Epoch 2, Batch: 360, Loss: 1.960772\n",
            "Epoch 2, Batch: 370, Loss: 1.994364\n",
            "Epoch 2, Batch: 380, Loss: 1.908645\n",
            "Epoch 2, Batch: 390, Loss: 1.953127\n",
            "Epoch 2, Batch: 400, Loss: 1.890761\n",
            "Epoch 2, Batch: 410, Loss: 1.970149\n",
            "Epoch 2, Batch: 420, Loss: 1.917663\n",
            "Epoch 2, Batch: 430, Loss: 1.903066\n",
            "Epoch 2, Batch: 440, Loss: 1.886718\n",
            "Epoch 2, Batch: 450, Loss: 1.845156\n",
            "Epoch 2, Batch: 460, Loss: 1.899533\n",
            "Epoch 2, Batch: 470, Loss: 1.923138\n",
            "Epoch 2, Batch: 480, Loss: 1.960658\n",
            "Epoch 2, Batch: 490, Loss: 1.905875\n",
            "Test Avg. Loss: 1.908849, Accuracy: 0.569200\n",
            "Epoch 3, Batch: 0, Loss: 1.928438\n",
            "Epoch 3, Batch: 10, Loss: 1.885843\n",
            "Epoch 3, Batch: 20, Loss: 1.895831\n",
            "Epoch 3, Batch: 30, Loss: 1.937122\n",
            "Epoch 3, Batch: 40, Loss: 1.933778\n",
            "Epoch 3, Batch: 50, Loss: 1.924128\n",
            "Epoch 3, Batch: 60, Loss: 1.893417\n",
            "Epoch 3, Batch: 70, Loss: 1.872451\n",
            "Epoch 3, Batch: 80, Loss: 1.984040\n",
            "Epoch 3, Batch: 90, Loss: 1.999869\n",
            "Epoch 3, Batch: 100, Loss: 1.877189\n",
            "Epoch 3, Batch: 110, Loss: 1.955627\n",
            "Epoch 3, Batch: 120, Loss: 1.896390\n",
            "Epoch 3, Batch: 130, Loss: 1.958820\n",
            "Epoch 3, Batch: 140, Loss: 1.920830\n",
            "Epoch 3, Batch: 150, Loss: 1.859414\n",
            "Epoch 3, Batch: 160, Loss: 1.840413\n",
            "Epoch 3, Batch: 170, Loss: 1.922471\n",
            "Epoch 3, Batch: 180, Loss: 1.843409\n",
            "Epoch 3, Batch: 190, Loss: 1.889235\n",
            "Epoch 3, Batch: 200, Loss: 1.841858\n",
            "Epoch 3, Batch: 210, Loss: 1.992708\n",
            "Epoch 3, Batch: 220, Loss: 1.880091\n",
            "Epoch 3, Batch: 230, Loss: 1.918527\n",
            "Epoch 3, Batch: 240, Loss: 1.926576\n",
            "Epoch 3, Batch: 250, Loss: 1.909935\n",
            "Epoch 3, Batch: 260, Loss: 1.782021\n",
            "Epoch 3, Batch: 270, Loss: 1.860350\n",
            "Epoch 3, Batch: 280, Loss: 1.883028\n",
            "Epoch 3, Batch: 290, Loss: 1.890187\n",
            "Epoch 3, Batch: 300, Loss: 1.920463\n",
            "Epoch 3, Batch: 310, Loss: 1.888735\n",
            "Epoch 3, Batch: 320, Loss: 1.862121\n",
            "Epoch 3, Batch: 330, Loss: 1.891335\n",
            "Epoch 3, Batch: 340, Loss: 1.847830\n",
            "Epoch 3, Batch: 350, Loss: 1.884164\n",
            "Epoch 3, Batch: 360, Loss: 1.910182\n",
            "Epoch 3, Batch: 370, Loss: 1.951481\n",
            "Epoch 3, Batch: 380, Loss: 1.863620\n",
            "Epoch 3, Batch: 390, Loss: 1.894464\n",
            "Epoch 3, Batch: 400, Loss: 1.864092\n",
            "Epoch 3, Batch: 410, Loss: 1.823727\n",
            "Epoch 3, Batch: 420, Loss: 1.907359\n",
            "Epoch 3, Batch: 430, Loss: 1.870917\n",
            "Epoch 3, Batch: 440, Loss: 1.846948\n",
            "Epoch 3, Batch: 450, Loss: 1.808750\n",
            "Epoch 3, Batch: 460, Loss: 1.839526\n",
            "Epoch 3, Batch: 470, Loss: 1.905656\n",
            "Epoch 3, Batch: 480, Loss: 1.901395\n",
            "Epoch 3, Batch: 490, Loss: 1.916785\n",
            "Test Avg. Loss: 1.886822, Accuracy: 0.574900\n",
            "Epoch 4, Batch: 0, Loss: 1.883882\n",
            "Epoch 4, Batch: 10, Loss: 1.912446\n",
            "Epoch 4, Batch: 20, Loss: 1.834837\n",
            "Epoch 4, Batch: 30, Loss: 1.894816\n",
            "Epoch 4, Batch: 40, Loss: 1.855189\n",
            "Epoch 4, Batch: 50, Loss: 1.824381\n",
            "Epoch 4, Batch: 60, Loss: 1.866359\n",
            "Epoch 4, Batch: 70, Loss: 1.875778\n",
            "Epoch 4, Batch: 80, Loss: 1.896021\n",
            "Epoch 4, Batch: 90, Loss: 1.946720\n",
            "Epoch 4, Batch: 100, Loss: 1.882900\n",
            "Epoch 4, Batch: 110, Loss: 1.959929\n",
            "Epoch 4, Batch: 120, Loss: 1.866959\n",
            "Epoch 4, Batch: 130, Loss: 1.848434\n",
            "Epoch 4, Batch: 140, Loss: 1.896069\n",
            "Epoch 4, Batch: 150, Loss: 1.933130\n",
            "Epoch 4, Batch: 160, Loss: 1.804302\n",
            "Epoch 4, Batch: 170, Loss: 1.835947\n",
            "Epoch 4, Batch: 180, Loss: 1.907015\n",
            "Epoch 4, Batch: 190, Loss: 1.818534\n",
            "Epoch 4, Batch: 200, Loss: 1.887846\n",
            "Epoch 4, Batch: 210, Loss: 1.862291\n",
            "Epoch 4, Batch: 220, Loss: 1.851193\n",
            "Epoch 4, Batch: 230, Loss: 1.816668\n",
            "Epoch 4, Batch: 240, Loss: 1.809465\n",
            "Epoch 4, Batch: 250, Loss: 1.832249\n",
            "Epoch 4, Batch: 260, Loss: 1.864461\n",
            "Epoch 4, Batch: 270, Loss: 1.857126\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0f4af7c442fb>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprev_test_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-39e4a4453bf0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMESSAGE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdigest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/hmac.py\u001b[0m in \u001b[0;36mdigest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[1;32m    158\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "for dataset_name in dataset_names:\n",
        "    (train_set, train_dataloader), (test_set, test_dataloader) = build_data(dataset_name, my_transforms, batch_size=batch_size, train_size=train_size, test_size=test_size)\n",
        "    batch_sample, label = next(iter(train_dataloader))\n",
        "    sample = np.asarray(batch_sample[0,:,:,:])\n",
        "\n",
        "    for hidden_layer_size in hidden_layer_sizes:\n",
        "        model, loss_fn, optimizer, scheduler = build_model(sample.shape, hidden_layer_size, output_size, learning_rate, momentum)\n",
        "\n",
        "        experiment_dict = dict()\n",
        "        experiment_dict['exp_name'] = str(uuid.uuid4())\n",
        "        experiment_dict['learning_rate'] = learning_rate\n",
        "        experiment_dict['epochs'] = epochs\n",
        "        experiment_dict['batch_size'] = batch_size\n",
        "        experiment_dict['train_set_size'] = len(train_set)\n",
        "        experiment_dict['test_set_size'] = len(test_set)\n",
        "        experiment_dict['sample_shape'] = sample.shape\n",
        "        experiment_dict['dataset_name'] = dataset_name\n",
        "        experiment_dict['transforms'] = str(my_transforms)\n",
        "        experiment_dict['model_summary'] = str(model)\n",
        "        experiment_dict['loss'] = 'cross_entropy'\n",
        "        pprint(experiment_dict)\n",
        "\n",
        "        print('run training..')\n",
        "        prev_test_loss = 0\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = train(epoch)\n",
        "            test_loss = test()\n",
        "\n",
        "            scheduler.step()\n",
        "            if np.abs(prev_test_loss - test_loss) < 0.000001:\n",
        "              print('constant test loss!')\n",
        "              break\n",
        "            prev_test_loss = test_loss\n",
        "\n",
        "        # save_history(history, data_dir=data_dir, experiment_dict=experiment_dict)\n",
        "        experiment_dict['hidden_layer_size'] = hidden_layer_size\n",
        "        experiment_dict['final_train_loss'] = train_loss\n",
        "        experiment_dict['final_test_loss'] = test_loss\n",
        "        experiment_dict['convergence_epoch'] = epoch\n",
        "\n",
        "        with open(path.join(data_dir, 'hidden_layer_loss.yml'),'r') as file:\n",
        "          experiments = yaml.safe_load(file)\n",
        "          exp_name = experiment_dict.pop('exp_name')\n",
        "          experiments[exp_name] = experiment_dict\n",
        "\n",
        "        with open(path.join(data_dir, 'hidden_layer_loss.yml'), 'w') as file:\n",
        "          yaml.safe_dump(experiments, file)\n",
        "        print('file saved')\n",
        "\n",
        "print('done')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}